{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htW5SiGzeXYm"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HGeUNoteaSm"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ3UDciDVcB5"
   },
   "source": [
    "# Bayesian Gaussian Mixture Model and Hamiltonian MCMC\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lin40yCC6eBo"
   },
   "source": [
    "\n",
    "In this colab we'll explore sampling from the posterior of a Bayesian Gaussian Mixture Model (BGMM) using only TensorFlow Probability primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZs1ShikNBK2"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JjokKMbk2hJ"
   },
   "source": [
    "For $k\\in\\{1,\\ldots, K\\}$ mixture components each of dimension $D$, we'd like to model $i\\in\\{1,\\ldots,N\\}$ iid samples using the following Bayesian Gaussian Mixture Model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta &\\sim \\text{Dirichlet}(\\text{concentration}=\\alpha_0)\\\\\n",
    "\\mu_k &\\sim \\text{Normal}(\\text{loc}=\\mu_{0k}, \\text{scale}=I_D)\\\\\n",
    "T_k &\\sim \\text{Wishart}(\\text{df}=5, \\text{scale}=I_D)\\\\\n",
    "Z_i &\\sim \\text{Categorical}(\\text{probs}=\\theta)\\\\\n",
    "Y_i &\\sim \\text{Normal}(\\text{loc}=\\mu_{z_i}, \\text{scale}=T_{z_i}^{-1/2})\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iySRABi0qZnQ"
   },
   "source": [
    "Note, the `scale` arguments all have `cholesky` semantics. We use this convention because it is that of TF Distributions (which itself uses this convention in part because it is computationally advantageous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6X_Beihwzyi"
   },
   "source": [
    "Our goal is to generate samples from the posterior:\n",
    "\n",
    "$$p\\left(\\theta, \\{\\mu_k, T_k\\}_{k=1}^K \\Big| \\{y_i\\}_{i=1}^N, \\alpha_0, \\{\\mu_{ok}\\}_{k=1}^K\\right)$$\n",
    "\n",
    "Notice that $\\{Z_i\\}_{i=1}^N$ is not present--we're interested in only those random variables which don't scale with $N$.  (And luckily there's a TF distribution which handles marginalizing out $Z_i$.)\n",
    "\n",
    "It is not possible to directly sample from this distribution owing to a computationally intractable normalization term.\n",
    "\n",
    "[Metropolis-Hastings algorithms](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) are technique for for sampling from intractable-to-normalize distributions.\n",
    "\n",
    "TensorFlow Probability offers a number of MCMC options, including several based on Metropolis-Hastings. In this notebook, we'll use [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)  (`tfp.mcmc.HamiltonianMonteCarlo`). HMC is often a good choice because it can converge rapidly, samples the state space jointly (as opposed to coordinatewise), and leverages one of TF's virtues: automatic differentiation. That said, sampling from a BGMM posterior might actually be better done by other approaches, e.g., [Gibb's sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fK2LHcyC6gCi"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uswTWdgNu46j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovNsKD-OEUzR"
   },
   "outputs": [],
   "source": [
    "def session_options(enable_gpu_ram_resizing=True):\n",
    "  \"\"\"Convenience function which sets common `tf.Session` options.\"\"\"\n",
    "  config = tf.ConfigProto()\n",
    "  config.log_device_placement = True\n",
    "  if enable_gpu_ram_resizing:\n",
    "    # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "    # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "    config.gpu_options.allow_growth = True\n",
    "  return config\n",
    "\n",
    "def reset_sess(config=None):\n",
    "  \"\"\"Convenience function to create the TF graph and session, or reset them.\"\"\"\n",
    "  if config is None:\n",
    "    config = session_options()\n",
    "  tf.reset_default_graph()\n",
    "  global sess\n",
    "  try:\n",
    "    sess.close()\n",
    "  except:\n",
    "    pass\n",
    "  sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "reset_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uj9uHZN2yUqz"
   },
   "source": [
    "Before actually building the model, we'll need to define a new type of distribution.  From the model specification above, its clear we're parameterizing the MVN with an inverse covariance matrix, i.e.,  [precision matrix](https://en.wikipedia.org/wiki/Precision_(statistics%29).  To accomplish this in TF,  we'll need to roll out our `Bijector`.  This `Bijector` will use the forward transformation:\n",
    "\n",
    "- `Y =`  [`tf.matrix_triangular_solve`](https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve)`(tf.matrix_transpose(chol_precision_tril), X, adjoint=True) + loc`.\n",
    "\n",
    "And the `log_prob` calculation is just the inverse, i.e.:\n",
    "\n",
    "- `X =` [`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/matmul)`(chol_precision_tril, X - loc, adjoint_a=True)`.\n",
    "\n",
    "Since all we need for HMC is `log_prob`, this means we avoid ever calling `tf.matrix_triangular_solve` (as would be the case for `tfd.MultivariateNormalTriL`). This is advantageous since `tf.matmul` is usually faster owing to better cache locality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nc4yy6vW-lC_"
   },
   "outputs": [],
   "source": [
    "class MVNCholPrecisionTriL(tfd.TransformedDistribution):\n",
    "  \"\"\"MVN from loc and (Cholesky) precision matrix.\"\"\"\n",
    "\n",
    "  def __init__(self, loc, chol_precision_tril, name=None):\n",
    "    super(MVNCholPrecisionTriL, self).__init__(\n",
    "        distribution=tfd.Independent(tfd.Normal(tf.zeros_like(loc),\n",
    "                                                scale=tf.ones_like(loc)),\n",
    "                                     reinterpreted_batch_ndims=1),\n",
    "        bijector=tfb.Chain([\n",
    "            tfb.Affine(shift=loc),\n",
    "            tfb.Invert(tfb.Affine(scale_tril=chol_precision_tril,\n",
    "                                  adjoint=True)),\n",
    "        ]),\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDOkWhDQg4ZG"
   },
   "source": [
    "The `tfd.Independent` distribution turns independent draws of one distribution, into a multivariate distribution with statistically independent coordinates. In terms of computing `log_prob`, this \"meta-distribution\" manifests as a simple sum over the event dimension(s).\n",
    "\n",
    "Also notice that we took the `adjoint` (\"transpose\") of the scale matrix. This is because if precision is inverse covariance, i.e., $P=C^{-1}$ and if $C=AA^\\top$, then $P=BB^{\\top}$ where $B=A^{-\\top}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pfkc8cmhh2Qz"
   },
   "source": [
    "Since this distribution is kind of tricky, let's quickly verify that our `MVNCholPrecisionTriL` works as we think it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "height": 153.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 507.0,
     "status": "ok",
     "timestamp": 1.538760458123E12,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420.0
    },
    "id": "GhqbjwlIh1Vn",
    "outputId": "d9c48cc8-5dd2-45c9-9ccc-3be0da3619ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean: [ 1. -1.]\nsample mean: [ 1.0002652 -1.0001206]\ntrue cov:\n [[ 1.0625   -0.03125 ]\n [-0.03125   0.015625]]\nsample cov:\n [[ 1.0641271  -0.03126179]\n [-0.03126179  0.0155931 ]]\n"
     ]
    }
   ],
   "source": [
    "def compute_sample_stats(d, seed=42, n=int(1e6)):\n",
    "  x = d.sample(n, seed=seed)\n",
    "  sample_mean = tf.reduce_mean(x, axis=0, keepdims=True)\n",
    "  s = x - sample_mean\n",
    "  sample_cov = tf.matmul(s, s, adjoint_a=True) / tf.cast(n, s.dtype)\n",
    "  sample_scale = tf.cholesky(sample_cov)\n",
    "  sample_mean = sample_mean[0]\n",
    "  return [\n",
    "      sample_mean,\n",
    "      sample_cov,\n",
    "      sample_scale,\n",
    "  ]\n",
    "\n",
    "dtype = np.float32\n",
    "true_loc = np.array([1., -1.], dtype=dtype)\n",
    "true_chol_precision = np.array([[1., 0.],\n",
    "                                [2., 8.]],\n",
    "                               dtype=dtype)\n",
    "true_precision = np.matmul(true_chol_precision, true_chol_precision.T)\n",
    "true_cov = np.linalg.inv(true_precision)\n",
    "\n",
    "d = MVNCholPrecisionTriL(\n",
    "    loc=true_loc,\n",
    "    chol_precision_tril=true_chol_precision)\n",
    "\n",
    "[\n",
    "    sample_mean_,\n",
    "    sample_cov_,\n",
    "    sample_scale_,\n",
    "] = sess.run(compute_sample_stats(d))\n",
    "\n",
    "print('true mean:', true_loc)\n",
    "print('sample mean:', sample_mean_)\n",
    "print('true cov:\\n', true_cov)\n",
    "print('sample cov:\\n', sample_cov_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N60z8scN1v6E"
   },
   "source": [
    "Since the sample mean and covariance are close to the true mean and covariance, it seems like the distribution is correctly implemented. Now, we'll use `MVNCholPrecisionTriL` and  stock`tfp.distributions` to specify the BGMM prior random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhzxySDjL2-S"
   },
   "outputs": [],
   "source": [
    "dtype = np.float32\n",
    "dims = 2\n",
    "components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAOmHhZ7LzDQ"
   },
   "outputs": [],
   "source": [
    "rv_mix_probs = tfd.Dirichlet(\n",
    "    concentration=np.ones(components, dtype) / 10.,\n",
    "    name='rv_mix_probs')\n",
    "\n",
    "rv_loc = tfd.Independent(\n",
    "    tfd.Normal(\n",
    "        loc=np.stack([\n",
    "            -np.ones(dims, dtype),\n",
    "            np.zeros(dims, dtype),\n",
    "            np.ones(dims, dtype),\n",
    "        ]),\n",
    "        scale=tf.ones([components, dims], dtype)),\n",
    "    reinterpreted_batch_ndims=1,\n",
    "    name='rv_loc')\n",
    "\n",
    "rv_precision = tfd.Wishart(\n",
    "    df=5,\n",
    "    scale_tril=np.stack([np.eye(dims, dtype=dtype)]*components),\n",
    "    input_output_cholesky=True,\n",
    "    name='rv_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "height": 88.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 244.0,
     "status": "ok",
     "timestamp": 1.538760469863E12,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420.0
    },
    "id": "KSTp8aAIAv0O",
    "outputId": "9b18cf98-8514-471b-a56f-429be5b30d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Dirichlet(\"rv_mix_probs/\", batch_shape=(), event_shape=(3,), dtype=float32)\ntfp.distributions.Independent(\"rv_loc/\", batch_shape=(3,), event_shape=(2,), dtype=float32)\ntfp.distributions.Wishart(\"rv_precision/\", batch_shape=(3,), event_shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(rv_mix_probs)\n",
    "print(rv_loc)\n",
    "print(rv_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZOG0OR815Nr"
   },
   "source": [
    "Using the three random variables defined above, we can now specify the joint log probability function. To do this we'll use `tfd.MixtureSameFamily` to automatically integrate out the categorical $\\{Z_i\\}_{i=1}^N$ draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpLnRJr2TXYD"
   },
   "outputs": [],
   "source": [
    "def joint_log_prob(observations, mix_probs, loc, chol_precision):\n",
    "  \"\"\"BGMM with priors: loc=Normal, precision=Inverse-Wishart, mix=Dirichlet.\n",
    "\n",
    "  Args:\n",
    "    observations: `[n, d]`-shaped `Tensor` representing Bayesian Gaussian\n",
    "      Mixture model draws. Each sample is a length-`d` vector.\n",
    "    mix_probs: `[K]`-shaped `Tensor` representing random draw from\n",
    "      `SoftmaxInverse(Dirichlet)` prior.\n",
    "    loc: `[K, d]`-shaped `Tensor` representing the location parameter of the\n",
    "      `K` components.\n",
    "    chol_precision: `[K, d, d]`-shaped `Tensor` representing `K` lower\n",
    "      triangular `cholesky(Precision)` matrices, each being sampled from\n",
    "      a Wishart distribution.\n",
    "\n",
    "  Returns:\n",
    "    log_prob: `Tensor` representing joint log-density over all inputs.\n",
    "  \"\"\"\n",
    "  rv_observations = tfd.MixtureSameFamily(\n",
    "      mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
    "      components_distribution=MVNCholPrecisionTriL(\n",
    "          loc=loc,\n",
    "          chol_precision_tril=chol_precision))\n",
    "  log_prob_parts = [\n",
    "      rv_observations.log_prob(observations), # Sum over samples.\n",
    "      rv_mix_probs.log_prob(mix_probs)[..., tf.newaxis],\n",
    "      rv_loc.log_prob(loc),                   # Sum over components.\n",
    "      rv_precision.log_prob(chol_precision),  # Sum over components.\n",
    "  ]\n",
    "  sum_log_prob = tf.reduce_sum(tf.concat(log_prob_parts, axis=-1), axis=-1)\n",
    "  # Note: for easy debugging, uncomment the following:\n",
    "  # sum_log_prob = tf.Print(sum_log_prob, log_prob_parts)\n",
    "  return sum_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1idLJazkGC"
   },
   "source": [
    "Notice that this function internally defines a new random variable. This is necessary since the `observations` RV depends on samples from the RVs defined further above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jTMXdymV1QJ"
   },
   "source": [
    "## Generate \"Training\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl4brz3G3pS7"
   },
   "source": [
    "For this demo, we'll sample some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AJZAtwXV8RQ"
   },
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "true_loc = np.array([[-2, -2],\n",
    "                     [0, 0],\n",
    "                     [2, 2]], dtype)\n",
    "random = np.random.RandomState(seed=42)\n",
    "\n",
    "true_hidden_component = random.randint(0, components, num_samples)\n",
    "observations = (true_loc[true_hidden_component] +\n",
    "                random.randn(num_samples, dims).astype(dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVOvMh7MV37A"
   },
   "source": [
    "## Bayesian Inference using HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdN3iKFT32Jp"
   },
   "source": [
    "Now that we've used TFD to specify our model and obtained some observed data, we have all the necessary pieces to run HMC.\n",
    "\n",
    "To do this, we'll use a [partial application](https://en.wikipedia.org/wiki/Partial_application) to \"pin down\" the things we don't want to sample. In this case that means we need only pin down `observations`. (The hyper-parameters are already baked in to the prior distributions and not part of the `joint_log_prob` function signature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVoaDFSf7L_j"
   },
   "outputs": [],
   "source": [
    "unnormalized_posterior_log_prob = functools.partial(joint_log_prob, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0OMIWIYeMmQ"
   },
   "outputs": [],
   "source": [
    "initial_state = [\n",
    "    tf.fill([components],\n",
    "            value=np.array(1. / components, dtype),\n",
    "            name='mix_probs'),\n",
    "    tf.constant(np.array([[-2, -2],\n",
    "                          [0, 0],\n",
    "                          [2, 2]], dtype),\n",
    "                name='loc'),\n",
    "    tf.eye(dims, batch_shape=[components], dtype=dtype, name='chol_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVpiT3LLyfcO"
   },
   "source": [
    "### Unconstrained Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS8XOsxiyiBV"
   },
   "source": [
    "Hamiltonian Monte Carlo (HMC) requires the target log-probability function be differentiable with respect to its arguments.  Furthermore, HMC can exhibit dramatically higher statistical efficiency if the state-space is unconstrained.\n",
    "\n",
    "This means we'll have to work out two main issues when sampling from the BGMM posterior:\n",
    "\n",
    "1. $\\theta$ represents a discrete probability vector, i.e., must be such that $\\sum_{k=1}^K \\theta_k = 1$ and $\\theta_k>0$.\n",
    "2. $T_k$ represents an inverse covariance matrix, i.e., must be such that $T_k \\succ 0$, i.e., is [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt9SXJzO0Cks"
   },
   "source": [
    "To address this requirement we'll need to:\n",
    "\n",
    "1. transform the constrained variables to an unconstrained space\n",
    "2. run the MCMC in unconstrained space\n",
    "3. transform the unconstrained variables back to the constrained space.\n",
    "\n",
    "As with `MVNCholPrecisionTriL`, we'll use [`Bijector`s](https://www.tensorflow.org/api_docs/python/tf/distributions/bijectors/Bijector) to transform random variables to unconstrained space.\n",
    "\n",
    "- The [`Dirichlet`](https://en.wikipedia.org/wiki/Dirichlet_distribution) is transformed to unconstrained space via the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "\n",
    "- Our precision random variable is a distribution over postive semidefinite matrices. To unconstrain these we'll use the `FillTriangular` and `TransformDiagonal` bijectors.  These convert vectors to lower-triangular matrices and ensure the diagonal is positive. The former is useful because it enables sampling only $d(d+1)/2$ floats rather than $d^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_atEQrDR7JvG"
   },
   "outputs": [],
   "source": [
    "unconstraining_bijectors = [\n",
    "    tfb.SoftmaxCentered(),\n",
    "    tfb.Identity(),\n",
    "    tfb.Chain([\n",
    "        tfb.TransformDiagonal(tfb.Softplus()),\n",
    "        tfb.FillTriangular(),\n",
    "    ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zq6QJJ-NSPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\bijectors\\softmax_centered.py:158: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "[mix_probs, loc, chol_precision], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=2000,\n",
    "    num_burnin_steps=500,\n",
    "    current_state=initial_state,\n",
    "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "            step_size=0.065,\n",
    "            num_leapfrog_steps=5),\n",
    "        bijector=unconstraining_bijectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ceX1A3-ZFiN"
   },
   "outputs": [],
   "source": [
    "acceptance_rate = tf.reduce_mean(tf.to_float(kernel_results.inner_results.is_accepted))\n",
    "mean_mix_probs = tf.reduce_mean(mix_probs, axis=0)\n",
    "mean_loc = tf.reduce_mean(loc, axis=0)\n",
    "mean_chol_precision = tf.reduce_mean(chol_precision, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmpTFZcVmByb"
   },
   "source": [
    "Note: through trial-and-error we've predetermined the `step_size` and `num_leapfrog_steps` to approximately achieve an [asymptotically optimal rate of 0.651](https://arxiv.org/abs/1001.4460). For a technique to do this automatically, see the examples section in `help(tfp.mcmc.HamiltonianMonteCarlo)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLEz96mg6fpZ"
   },
   "source": [
    "We'll now execute the chain and print the posterior means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3B2yJWVmNcrm"
   },
   "outputs": [],
   "source": [
    "[\n",
    "    acceptance_rate_,\n",
    "    mean_mix_probs_,\n",
    "    mean_loc_,\n",
    "    mean_chol_precision_,\n",
    "    mix_probs_,\n",
    "    loc_,\n",
    "    chol_precision_,\n",
    "] = sess.run([\n",
    "    acceptance_rate,\n",
    "    mean_mix_probs,\n",
    "    mean_loc,\n",
    "    mean_chol_precision,\n",
    "    mix_probs,\n",
    "    loc,\n",
    "    chol_precision,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "height": 306.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 257.0,
     "status": "ok",
     "timestamp": 1.538760907401E12,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420.0
    },
    "id": "bqJ6RSJxegC6",
    "outputId": "25a86519-852f-4368-f8d4-2914ff1db494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    acceptance_rate: 0.671\n      avg mix probs: [0.3823643  0.26740772 0.35022804]\n\n            avg loc:\n [[-1.887417   -1.8008987 ]\n [-0.01032159  0.0223233 ]\n [ 1.9130157   1.9374167 ]]\n\navg chol(precision):\n [[[ 1.0085353   0.        ]\n  [-0.05432986  0.9732314 ]]\n\n [[ 1.2092963   0.        ]\n  [ 0.1903022   1.0157472 ]]\n\n [[ 0.99330944  0.        ]\n  [-0.1047408   0.97317016]]]\n"
     ]
    }
   ],
   "source": [
    "print('    acceptance_rate:', acceptance_rate_)\n",
    "print('      avg mix probs:', mean_mix_probs_)\n",
    "print('\\n            avg loc:\\n', mean_loc_)\n",
    "print('\\navg chol(precision):\\n', mean_chol_precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "height": 286.0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11557.0,
     "status": "ok",
     "timestamp": 1.538760926241E12,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420.0
    },
    "id": "zFOU0j9kPdUy",
    "outputId": "187b368d-0fb7-4e49-88b5-e215f4a27d9d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGItJREFUeJzt3X9s1PXhx/FXbfl5pfR6HYVra6FFVn4IC+NHaIJF1C0x\nzIAhBRxkxqlRJhkypsL4OQYuiw0mEyVuzB8s21Ib0UwXGcrECSwoWJwSBiJooa5dWyq/Wkrp+/uH\n3zvurtdf3Kf9XN99PhJj7z53n3v3HXj2zft+NMEYYwQAsNINbg8AANB1iDwAWIzIA4DFiDwAWIzI\nA4DFiDwAWIzIo1d47rnnlJGRoeTkZNXU1IQdO3XqlBISEtTU1NSlYxg+fLjefvvtLn0MIBKRR5eI\nDNpf/vIXeb1e7dmzJxjV5ORkJScnKyMjQ7NmzdKuXbtanGPAgAHB2yUnJ+uRRx7p9FiuXLmiZcuW\n6e9//7suXLggn88X8/cH9BREHl3upZde0k9+8hO9+eabKiwsDF5fV1enCxcu6PDhw7rjjjs0Z84c\nvfjii2H3/etf/6oLFy4E/3vmmWc6/fiVlZVqaGjQ2LFjY/1WukxX/ysCvReRR5d6/vnn9bOf/Uw7\nd+5UQUFB1NsMHTpUP/3pT7Vu3To9/vjjam5u7vTjXL58WUuXLpXf75ff79fSpUt1+fJlHTt2TN/+\n9rclSampqZo5c2a756qoqNBdd92ltLQ0jRw5Ur/73e+Cx65evapNmzYpLy9PgwYN0ne/+12Vl5dH\nPc/27duVk5Mjn8+njRs3hh1bt26d5s6dq4ULFyolJUUvvviiDhw4oGnTpik1NVXDhg3TI488osbG\nRknS2rVrtWTJEknf/MvE4/HosccekyTV19erf//+Onv2rBoaGrRw4UL5fD6lpqZq8uTJqqys7PR8\nwiIG6AI5OTnm7rvvNkOGDDFlZWVhx06ePGkkmStXroRdf+LECSPJHDlyJHiOXbt2dejxVq9ebaZO\nnWoqKytNVVWVmTZtmlm1alWbj9faeG655Rbz8MMPm/r6evPRRx+Z9PR08/bbbxtjjPnNb35jxo0b\nZ44ePWqam5tNWVmZqa6ubnHOTz/91Hg8HrNnzx7T0NBgHn30UZOYmBj8ftauXWuSkpLMjh07zNWr\nV82lS5fMhx9+aPbv32+uXLliTp48afLz883mzZuNMca88847Zty4ccYYY/bu3Wtyc3PNlClTgsfG\njx9vjDFm69atZtasWebixYumqanJfPjhh+brr7/u0BzCTkQeXSInJ8cMGjTI3HXXXebq1athx1qL\nbn19vZFk3n///eA5PB6PGTx4cPC/559/Purj5ebmmjfffDN4+a233jI5OTltPl608Xz55Zfmhhtu\nMOfOnQsef+KJJ8yPfvQjY4wxo0aNMq+99lq73//69evNvHnzgpcvXLhg+vTpExb56dOnt3mOzZs3\nm9mzZxtjjLl06ZLp16+fqa6uNk8++aTZuHGjyczMNOfPnzdr1qwxS5YsMcYYs23bNjNt2jRz+PDh\ndseI3oHtGnSZrVu36tixY7r//vtlOvA5eGfOnJEkpaWlBa977bXXVFdXF/zvgQceiHrfiooK5eTk\nBC/n5OSooqKi02OuqKhQWlqaBg0aFHauwNjKy8uVl5fXofNkZ2cHL3s8nhZP+IYel6Rjx45p1qxZ\nGjp0qFJSUrRy5UpVV1dLkgYMGKBJkyZpz549eu+991RYWKiCggLt3btXe/bsCT7XsWjRIn3/+9/X\n/Pnz5ff79dhjj+nKlSudngfYg8ijywwZMkTvvPOO/vnPf2rx4sXt3n7Hjh0aMmRIcA+9M/x+v774\n4ovg5S+//FJ+v/+6zlNbW6vz58+HnSszM1PSN2E+ceJEu+cZNmxY2F79pUuXWrx0MyEhIezyww8/\nrPz8fB0/flznzp3Tpk2bwn44FhYWavfu3froo480efJkFRYWaufOnTpw4IBuueUWSVKfPn20du1a\nHTlyRPv27dMbb7yhl19+udPzAHsQeXQpv9+v3bt366233tKjjz4a9TaVlZV65plntH79ej355JO6\n4YbO/7FcsGCBfvWrX+l///ufqqur9ctf/lILFy7s9Hmys7NVUFCgFStWqKGhQR9//LG2bdumH/7w\nh5Kk+++/X6tXr9bx48dljNHHH3/cIt6SNHfuXL3xxht6//331djYqDVr1rT7hPL58+eVkpKi5ORk\nHT16VM8991zY8cLCQr388ssaM2aM+vbtqxkzZuj3v/+9RowYoW9961uSpH/84x/697//ratXryol\nJUV9+vRRYmJip+cB9iDy6HLZ2dnavXu3SktLtWLFiuD1qamp8ng8uvnmm/W3v/1Nr7zyiu67776w\n+/7gBz8Ie538nDlzoj7GqlWrNGnSJI0fP14333yzJk6cqFWrVl3XeP/85z/r1KlT8vv9mjNnjtav\nX6877rhDkrRs2TIVFRXpe9/7nlJSUvTjH/9Y9fX1Lc4xduxYbdmyRffcc4+GDRsmr9errKysNh/3\nqaee0p/+9CcNGjRIDzzwgObNmxd2vKCgQPX19cFV+5gxY9S/f//gZUn673//q7lz5yolJUWjR49W\nYWHhdf2wgz0STEc2SwEAPRIreQCwGJEHAIsReQCwGJEHAIsReQCwGJEHAIsluT0ASdf19nOn+f3+\nuBhHPGAuwjEf4ZycD0/6EEfO0xsN7tuxfLOSBwCLEXkAsBiRBwCLEXkAsBiRBwCLEXkAsBiRBwCL\nEXkAsFhcvBkKAHqTusbGqNen9u3r+GMReQDoRoHAV1/+Ouz69H6DVdfY6Hjo2a4BgDgQGX2nEHkA\niAPp/QZ3yXnZrgEABwW2Y1rbdknt21d1jY0tot4V+/ESkQeA69baE6ihx6LFu6uCHg2RB4BOiBb2\nwH56tC2XaLcn8gAQR0JD3dYTpNFeMdPa+bor9EQeAFoR7eWOR+sqO3z//NRv/t9VT6p2RMyRr66u\n1pYtW1RXV6eEhATdfvvtuvPOO50YGwB0u2ir9kDYj5w9HfU+Y7xZwa+PnD0ddtltMUc+MTFRixYt\nUm5ururr6/XEE09o/PjxysqKn28SANrTXtxPnD0pSTpdc0ySlOUbJUnK844IC3tHAt+j9uS9Xq+8\nXq8kacCAAcrMzFRtbS2RB9BjhG7LRK7aQ+P+VfnR4H2+Kj+qYdn5klqGXpLyUzMktdyq6c7ASw7v\nyVdVVenkyZMaOXKkk6cFgC4Rued+tK4yatylb6J+prIm4gzXop/nHRH8OlrguzvuAY5FvqGhQcXF\nxbr33ns1cODANm9bUlKi0tLS4Nd+v9+pYcQkXsYRD5iLcMxHOKfm4+vGJkfOcz1aW71Hi7ukKIHv\nGRyJfFNTk4qLizV9+nRNnTq13dsXFRWpqKgoeLmiosKJYcTE7/fHxTjiAXMRjvkI5+R8eNKHOHKe\nzooMfOjqPda4B1bxodxaxUsORN4Yo61btyozM1OzZs1yYkwA0GWiBT4Q986EPTPDJ+mbJ2DzvCNa\nPOHq5ssmQ8Uc+f/85z967733dOONN+rnP/+5JGnBggWaOHFizIMDAKdEvnomWuDPVNbo8qm6qPfv\nNzw1+HUg8IEnXgPiaS8+IObI5+fnq6SkxImxAECXaO0J1sjAd0Rmhk/DsvPDXkI5xpvV6qtp3MY7\nXgFYra1X0ITKzPDpTGVN2Io98ni0uEvfrODdfqlka4g8ACu19eYm6dpLJEMFtmEidTTu8RL2UEQe\ngHXaC3ykyL31gNCwSwqLu3RtayYe4x5A5AFYpaOBz/OO0ImzJ4Mhj6Ynxz2AyAPodcZ4s3Tk7Omw\nd6lGHg8Ifd17vG/NREPkAVijtd/UlJ+a0WI1H+2DxCLfyBSvT6Z2BpEH0CtEeydqpGgvf+yJYQ9F\n5AFY63pes97Tox7pBrcHAABOiSXQqX37Whd4iZU8AMvYGOpYsJIHAIsReQCwGJEHAIsReQCwGJEH\nAIsReQCwGJEHAIsReQCwGJEHAIsReQCwGJEHAIsReQCwGJEHAIvxKZQAXHOxusrtIcTE7/eroqLC\nlcce7Pd36Has5AHAYkQeACxG5AHAYkQeACxG5AHAYkQeACxG5AHAYkQeACxG5AHAYkQeACxG5AHA\nYkQeACxG5AHAYo58CuWzzz6rQ4cOafDgwSouLnbilAAABziykp8xY4ZWrlzpxKkAAA5yJPJjxoxR\ncnKyE6cCADiIPXkAsJgrvxmqpKREpaWlwa/9HfwNJ10tXsYRD5iLcMxHOObjmnifC1ciX1RUpKKi\nouBlt359Vig3f41XvGEuwjEf4ZiPa9yci47+cGG7BgAs5shK/umnn9aRI0d0/vx5PfTQQyoqKtLM\nmTOdODUAIAaORH7p0qVOnAYA4DC2awDAYkQeACxG5AHAYkQeACxG5AHAYkQeACxG5AHAYkQeACxG\n5AHAYkQeACxG5AHAYkQeACzmyufJo3cZmD7E7SHEpK6xKW6/h0vVVW4PAXGOlTwAWIzIA4DFiDwA\nWIzIA4DFiDwAWIzIA4DFiDwAWIzIA4DFiDwAWIzIA4DFiDwAWIzIA4DFiDwAWIxPoQQ6ofZyQ9jl\ntH79XRoJ0DFEHmhDZNSjHSf0iGdEHoiivbi3ddvI6AeO88MAbiDyQITQaJdfrG5xPNuTHvVY4PrW\nfkCw6ocbeOIVCNFe4APXRzvW2u0BN7GSR6/XWtg/OVvR4rbjvP42j5VfrA6u6IF4QOTRqwUC31rc\nD1d/LkmakJ7b4hjQExB59FqRe+eBgAfCHiradYHwdxT78XADkUevV36xOizwp2qPtnuf4Wn5YZcD\n2zitIfBwC5FHrxa6TRO6Wj9T/mmr98nMHht2mcAjnhF59DrRtmkCgY9cxX9x+nTY5ZysrE49FoGH\n2xyJfFlZmV544QU1Nzfrtttu0+zZs504LeC4yCdaowU+sIqPDDzQE8Uc+ebmZm3btk2rVq2Sz+fT\nihUrNGnSJGV1csUDdLXWAh8Zd4nAwx4xvxnqs88+09ChQ5WRkaGkpCQVFBTogw8+cGJsgOOcDnzk\nfjyvkUe8iXklX1tbK5/PF7zs8/l0/PjxWE8LdJnQLZpIoYG/eLJWkuQZkdbuOaPFnf14xIOYI2+M\naXFdQkJCm/cpKSlRaWlp8Gu/v+1XJ3SXeBlHPHByLuoamxw71/Vq7wPHMrPH6kz5p8rJygoLfWjg\nO/qka3fG3a0/s/xduSbe5yLmyPt8PtXU1AQv19TUyOv1tnmfoqIiFRUVBS9XVLj/LkK/3x8X44gH\nTs/FwPQhjp3rekR7V2vA8LT84JZNIPQBHVnBR+ru1bsbf2b5u3KNm3PR0R8uMe/J5+Xl6auvvlJV\nVZWampq0b98+TZo0KdbTAl1qQnpu8B2rw9Pyg29uyswe22LFnpOVFbwuM3ushqflh73blX14xLOY\nV/KJiYm67777tHHjRjU3N+vWW29Vdna2E2MDYtbeNk0g1oerPw9b1UeGPvAGqNB3uo7z+sMCzx48\n4pEjr5OfOHGiJk6c6MSpgC7X1idJSi3f0Spdi3vgh0Jo4Ik74hnveIWVoq3gsz3pYfvyobGfkJ7b\nYjUvRY974FwSgUf8I/KwXlq//sHoR4Ze+ibckaEPiIx74BxAT0HkYaXIFXZnQh8pcvXe2mMA8YjI\no9cIRLn2ckOroY+GuKMnI/LotVr7hdyRx0MRePQ0RB69TujWjdT+HjthR09G5NErEW70FjG/4xUA\nEL+IPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWI\nPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWS3B4A7HepusrtIcTE7/eroqLC7WEA\n14WVPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMViivz+/fu1bNkyzZs3TydOnHBq\nTAAAh8QU+ezsbC1fvlyjR492ajwAAAfF9LEGWVlZTo0DANAF2JMHAIu1u5LfsGGD6urqWlw/f/58\nTZ48+boetKSkRKWlpcGv/X7/dZ3HafEyjnjAXIRjPsIxH9fE+1wkGGNMrCdZt26dFi1apLy8vOu6\nfzx8wh+fNHgNcxGO+QjHfFzj5lx09IcL2zUAYLGYIn/gwAE99NBDOnbsmH79619r48aNTo0LAOCA\nmF5dM2XKFE2ZMsWpsQAAHMZ2DQBYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABYjMgDgMWIPABY\njMgDgMVi+lgDpwxMH+L2EFTX2BQX44jFpeoqt4cAIM6wkgcAixF5ALAYkQcAixF5ALAYkQcAixF5\nALAYkQcAixF5ALAYkQcAixF5ALBYXHysQU9We7mh1WNp/fp340gAoCUifx3aCnu02xF7AG4h8p0Q\nGvfyi9VRb5PtSW/1fsQeQHcj8h3QkbhHOx4t+ADQnYh8GyLj/snZig7fd5zXHwx+IPa1lxtYzQPo\nVkQ+QuR+e2jcD1d/rlO1R1vcZ3haftjlCem5+uRshcZ5/V03UADoACL//6JtyUTG/Uz5py3ul5k9\nNhj+yNgDgNuIvK4FPjTuh6s/l6RgwKMFPlRHAs9WDYDu1qsj39qee+i2THtxjzQhPVeSgls1gf14\nAg/ADb028u2t3s+Uf6ovTp+Oet+crKyo1xN4APHG6si396aljgT+4snasPt4RqTpi9Onw0I/PC2f\nwAOIS732s2sin1yNJjLwkddlZo8N24sf5/Ur25NO4AHEDatX8tGEvlkpdA9eUouXR3pGpEVdyUvX\nAj8hPZfVO4C4ZXXk0/r1b3XLJjLwoTKzxwa//iLiWE5WVovAE3cA8crqyHdWtJdBhgY/cJvQ/XcC\nDyCeWRX5jn46ZEeExjwUWzMAepKYIr99+3YdPHhQSUlJysjI0OLFi+XxeJwaW5ca5/Xrk7MVwZAH\ntm0iwx760QSRHzhG4AHEu5giP378eN1zzz1KTEzUH//4R+3YsUMLFy50amxdItuTHnzyNRB6KTzu\nkZ85Q9wB9FQxRX7ChAnBr0eNGqV//etfMQ8oFoH4trdtE4h2+cXqNlfqrZ0fAHoKx/bkd+/erYKC\nAqdOF5PIGLcWfVboAGyXYIwxbd1gw4YNqqura3H9/PnzNXnyZEnSq6++qhMnTmj58uVKSEho90FL\nSkpUWloa/Lqusel6xt4pveF3sab2tep5dAAOaDfy7Xn33Xe1a9curVmzRv369buuc3RH5AMiY29L\n4CXpUnWVI+fx+/2qqOj4L0ixHfMRjvm4xs258Ps79vsqYlr6lZWV6fXXX9f69euvO/DdzaaoA0B7\nYor8tm3b1NTUpA0bNkiSbrrpJj344IOODAwAELuYIv/b3/7WqXEAALpAr/0USgDoDYg8AFiMyAOA\nxYg8AFiMyAOAxWJ+MxQAIH6xkv9/RUVFbg8hbjAX4ZiPcMzHNT1hLog8AFiMyAOAxRLXrVu3zu1B\nxANjjMaOHdv+DXsB5iIc8xGO+bimJ8wFT7wCgMXYrgEAixF5ALAYkQcAixF5ALAYkQcAi/Gbn0Ns\n375dBw8eVFJSkjIyMrR48WJ5PB63h+WK/fv365VXXtGZM2e0adMm5eXluT2kbldWVqYXXnhBzc3N\nuu222zR79my3h+SaZ599VocOHdLgwYNVXFzs9nBcV11drS1btqiurk4JCQm6/fbbdeedd7o9rKhY\nyYcYP368iouL9dRTT2nYsGHasWOH20NyTXZ2tpYvX67Ro0e7PRRXNDc3a9u2bVq5cqU2b96svXv3\n6vTp024PyzUzZszQypUr3R5G3EhMTNSiRYu0efNmbdy4UTt37ozbPx9EPsSECROUmJgoSRo1apRq\na2tdHpF7srKyOvzb4G302WefaejQocrIyFBSUpIKCgr0wQcfuD0s14wZM0bJycluDyNueL1e5ebm\nSpIGDBigzMzMuO0FkW/F7t279Z3vfMftYcAltbW18vl8wcs+ny9u/xLDXVVVVTp58qRGjhzp9lCi\n6nV78hs2bFBdXV2L6+fPn6/JkydLkl599VUlJiZq+vTp3T28btWRueitor0RPCEhwYWRIJ41NDSo\nuLhY9957rwYOHOj2cKLqdZFfvXp1m8ffffddHTx4UGvWrLH+L3V7c9Gb+Xw+1dTUBC/X1NTI6/W6\nOCLEm6amJhUXF2v69OmaOnWq28NpFds1IcrKyvT666/r8ccfV79+/dweDlyUl5enr776SlVVVWpq\natK+ffs0adIkt4eFOGGM0datW5WZmalZs2a5PZw28QFlIZYsWaKmpqbgE0w33XSTHnzwQZdH5Y4D\nBw7oD3/4g86dOyePx6Phw4frF7/4hdvD6laHDh3SSy+9pObmZt166626++673R6Sa55++mkdOXJE\n58+f1+DBg1VUVKSZM2e6PSzXHD16VGvWrNGNN94Y/Bf/ggULNHHiRJdH1hKRBwCLsV0DABYj8gBg\nMSIPABYj8gBgMSIPABYj8gBgMSIPABYj8gBgsf8DiK2n7Aq47XIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c806ec1438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.kdeplot(loc_[:,0,0], loc_[:,0,1], shade=True)\n",
    "ax = sns.kdeplot(loc_[:,1,0], loc_[:,1,1], shade=True)\n",
    "ax = sns.kdeplot(loc_[:,2,0], loc_[:,2,1], shade=True)\n",
    "plt.title('KDE of loc draws');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmfNIM1c6mwc"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8LeIeMn6ot4"
   },
   "source": [
    "This simple colab demonstrated how TensorFlow Probability primitives can be used to build hierarchical Bayesian mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayesian_Gaussian_Mixture_Model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
